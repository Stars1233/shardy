/* Copyright 2024 The Shardy Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

include "mlir/Pass/PassBase.td"

def ShardingConstraintToReshardPass : Pass<"sdy-sharding-constraint-to-reshard", "func::FuncOp"> {
  let summary = "Converts ShardingConstraintOp into ReshardOp.";
  let dependentDialects = ["mlir::sdy::SdyDialect"];
}

def SinkDataFlowEdgesPass : Pass<"sdy-sink-data-flow-edges", "func::FuncOp"> {
  let summary = "Sinks all `DataFlowEdgeOp` into their input.";
  let description = [{
    Moves the sharding of each `DataFlowEdgeOp` to its input (the root target of
    the edge), and replaces the op with its input.
  }];
  let dependentDialects = ["mlir::sdy::SdyDialect"];
  //TODO(tomnatan): consider moving the sharding to all targets that can have a sharding attached.

  let options = [
    Option<"sinkDebugShardingOrigins", "sink-debug-sharding-origins", "bool",
           /*default=*/"false",
           "Whether to sink the debug sharding origins info. See "
           "`debug-sharding-origins` option in propagation for more info.">,
    Option<"sinkDebugPropagationEdgeSharding",
           "sink-debug-propagation-edge-sharding", "bool",
           /*default=*/"false",
           "Whether to sink the debug propagation edge sharding info. See "
           "`debug-propagation-edge-sharding` option in propagation for more "
           "info.">
  ];
}

def UpdateNonDivisibleInputOutputShardingsPass : Pass<"sdy-update-non-divisible-input-output-shardings", "ModuleOp"> {
  let summary = "Makes FuncOp inputs/outputs evenly sharded, removing any need for padding due to non-divisible shardings.";
  let description = [{
    Users of Shardy expect the function inputs/outputs to be evenly
    divisible/shardable to avoid requiring padding their tensors. Propagation
    may make inputs/outputs have non-divisible shardings, so this pass updates
    them to the largest dimension sharding prefix of the original sharding that
    is evenly sharded.
  }];
  let dependentDialects = ["mlir::sdy::SdyDialect"];
}

def InsertExplicitReshardsPass : Pass<"sdy-insert-explicit-reshards", "func::FuncOp"> {
  let summary = "Inserts explicit reshards to make all operations have compatible shardings.";
  let description = [{
    A compatible sharding essentially means that the operation can accept the
    sharded operands and produce a sharded result without requiring any reshard
    communications (note that the operation might still require communication
    such as all-reduce or halo-swaps).

    After propagation, some operations may still have incompatible shardings.

    Note that when an axis (or sub-axis) is used to shard non-corresponding
    dimensions (e.g. non-contracting dimensions in matmul) across multiple
    tensors, or when an axis shards a dimension in one tensor but not the
    corresponding dimension in the other tensor, it is said that the operation
    has a sharding conflict. Hence, after this pass, the operations become
    conflict-free.

    This pass injects reshard operations explicitly so that, for each operation,
    corresponding dimensions become sharded in the same way across all operands
    and results, and every axis (or sub-axis) can only be used to shard a single
    dimension type.

    Example:

    Input:
    ```mlir
    mesh = <"x"=4, "y"=2>
    %lhs : tensor<8x32xf32> {sdy.sharding=<@mesh, \[{"x"}, {"y"}\]>}
    %rhs : tensor<32x16xf32> {sdy.sharding=<@mesh, \[{"y"}, {"x"}\]>}
    stablehlo.dot %lhs, %rhs {sdy.sharding_per_value=<[<@mesh, \[{"x"}, {}\]>]>}
      : (tensor<8x32xf32>, tensor<32x16xf32>) -> tensor<8x16xf32>
    ```

    Output:
    ```mlir
    sdy.mesh = <"x"=4, "y"=2>
    %lhs : tensor<8x32xf32> {sdy.sharding=<@mesh, \[{"x"}, {"y"}\]>}
    %rhs : tensor<32x16xf32> {sdy.sharding=<@mesh, \[{"y"}, {"x"}\]>}
    %0 = sdy.reshard %rhs <@mesh, \[{"y"}, {}\]> : tensor<32x16xf32>
    stablehlo.dot %lhs, %0 {sdy.sharding_per_value=<[<@mesh, \[{"x"}, {}\]>]>}
      : (tensor<8x32xf32>, tensor<32x16xf32>) -> tensor<8x16xf32>
    ```

    In the example above, `lhs` and `rhs` are both sharded on axis "x" on their
    non-contracting dimensions, which is incompatible. The pass inserts an
    explicit reshard on `rhs` before the dot operation, so that the dot
    operation has compatible shardings.
  }];
  let dependentDialects = ["mlir::sdy::SdyDialect"];
  // TODO(b/402429253): delete when explicit reshards is enabled by default.
  // The non-full version allows us to insert explicit reshards on specific
  // operations for optimizations.
  let options = [
      Option<"enableFullVersion", "enable-full-version",
            "bool", /*default=*/"false",
            "Enable full version.">,
      Option<"avoidReshardsOnNamedComputations",
            "avoid-reshards-on-named-computations",
            "bool", /*default=*/"false",
            "Avoid explicit reshards/collectives on named computations.">
    ];
}

def ReshardToCollectivesPass : Pass<"sdy-reshard-to-collectives", "func::FuncOp"> {
  let summary = "Converts ReshardOp into various Shardy collective ops.";
  let dependentDialects = ["mlir::sdy::SdyDialect"];
  let description = [{
    Matches reshard ops and rewrites them into various Shardy collective
    ops. After this pass, no reshard ops remain in the module.

    Optionally if `keepRedundantReshards` is true, the only reshard ops that
    remain are the redundant ones. By default it assumes that explicit reshards
    have already been inserted (`sdy-insert-explicit-reshards`) and does not
    keep redundant reshards. It should keep redundant reshards if explicit
    reshards may have not been already inserted.

    Example:

    Input:
    ```mlir
    mesh = <"x"=2, "y"=2, "z"=2>
    %0 : tensor<16x2xf32> {sdy.sharding<@mesh, \[{"x", "y", "z"}, {}\]>
    %1 = sdy.reshard %arg0 <@mesh, \[{"x"}, {}\]> : tensor<16x2xf32>
    ```

    Output:
    ```mlir
    mesh = <"x"=2, "y"=2, "z"=2>
    %0 : tensor<16x2xf32> {sdy.sharding<@mesh, \[{"x", "y", "z"}, {}\]>
    %1 = sdy.all_gather  \[{"y", "z"}, {}\] %arg0 out_sharding=<@mesh, \[{"x"}, {}\]> : tensor<16x2xf32>
    ```

    In the example above, the tensor `%0 : tensor<16x2xf32>` is sharded as
    `\[{"x", "y", "z"}, {}\]`. Then, there's a `reshard` op resharding it as
    `\[{"x"}, {}\]`. On the first axes, since the suffix `{"y", "z"}` is removed
    after the reshard, we infer that we have all-gathered `{"y", "z"}`. The
    second dimension is not changed.

  }];
  let dependentDialects = ["mlir::sdy::SdyDialect"];
  let options = [
      // TODO(enver): Drop keepRedundantReshard option once shardy partitioner
      // migration is complete.
      Option<"keepRedundantReshards", "keep-redundant-reshards",
            "bool", /*default=*/"false",
            "Whether it keeps redundant reshards or removes.">
    ];
}

def RemoveAllGatherReduceScatterForCMV1Pass : Pass<"sdy-remove-all-gather-reduce-scatter-for-cmv1", "func::FuncOp"> {
  let summary = "Removes sdy.all_gather and sdy.reduce_scatter for CMV1.";
  let dependentDialects = ["mlir::sdy::SdyDialect"];
  let description = [{
    Removes all-gather in the pattern all-gather + dot. Removes reduce-scatter
    in the pattern dot + reduce-scatter. This pass is for compatibility with
    collective matmul V1 (CMV1). It is a temporary solution for b/432019089.
  }];
}

def RemoveShardingGroupsPass : Pass<"sdy-remove-sharding-groups", "ModuleOp"> {
  let summary = "Removes ShardingGroupOps after propagation.";
  let dependentDialects = ["mlir::sdy::SdyDialect"];
}

def CloseShardingsPass : Pass<"sdy-close-shardings", "ModuleOp"> {
  let summary = "Closes tensor shardings and drops replicated axes.";
  let dependentDialects = ["mlir::sdy::SdyDialect"];
}

def DropShardingRulesPass : Pass<"sdy-drop-sharding-rules", "func::FuncOp"> {
  let summary = "Drops `OpShardingRuleAttr` from all registered ops.";
  let dependentDialects = ["mlir::sdy::SdyDialect"];
}

def ConstantOrScalarMergerPass : Pass<"sdy-constant-or-scalar-merger", "func::FuncOp"> {
  let summary = "Merge identical constants and scalar expansions with matching shardings.";
  let description = [{
    Performs a lightweight CSE on constants with identical shardings.

    The import pipeline splits and duplicates the constants and scalar
    expansions such that sharding is not propagated between different uses of a
    constant sub-computation. If the constants have same shardings after
    propagation, this pass merges them to save compilation time. See
    -sdy-constant-or-scalar-splitter for more info.
  }];
  let dependentDialects = ["mlir::sdy::SdyDialect"];
}

def RemovePropagationDebugInfoPass : Pass<"sdy-remove-propagation-debug-info", "ModuleOp"> {
  let summary = "Removes propagation debug info (propagation edges and origin shardings) during export.";
  let dependentDialects = ["mlir::sdy::SdyDialect"];
}
